
## 6.2 차원축소

### 6.2.1 주성분 분석

- SVD(singular value decomposition)을 통해 주성분 분석

- 고차원에서 주어진 데이터들을 임의의 주성분 고차원 평면(초평면)에 투사했을 때 투사점들 사이가 최대한(분산이 최대)

- 원래 벡터와 고차원 평면상의 투사된 거리가 최소가 되어야 함

- 높은 차원을 지나치게 낮은 차원으로 축소하여 표현하기 어려움

- 데이터가 비선형적으로 구성될수록 어려워짐

### 6.2.2 매니폴드 가설

- 높은 차원에 존재하는 데이터는 해당 데이터들을 아우르는 낮은 차원의 다양체(manifold)가 존재

- 저차원 매핑시 가까워지는 점끼리 비슷한 feature를 갖기도 함

### 6.2.3 딥러닝이 잘 동작한 이유

- 비선형적인 방식으로 차원축소-> 해당 문제를 잘 해결하기 위한 매니폴드를 자연스럽게 찾아냄

### 6.2.4 오토인코더

- 고차원의 샘플 벡터를 입력으로 받아 매니폴드를 찾고, 저차원으로 축소하는 인코더를 거쳐 bottle-neck구간에서의 hidden vector로 표현

- 디코더는 저차원의 벡터를 받아 입력 샘플이 존재하던 고차원으로 데이터를 복원하는 작업 수행

	- 복원된 데이터와 실제 입력 사이의 차이를 최소화하도록 손실함수를 구성

- TF-IDF 희소단어특징 벡터를 입, 출력으로 쓰는 모델을 사용시켜서 bottle neck결과값을 임베딩 벡터로 사용할 수 있을 것

## 6.3 흔한 오해

문제의 특징을 고려하지 않은 단어 임베딩 벡터는 그다지 좋은 방법이 아님  

### 6.3.1 word2vec 없이 신경망 훈련 

- 실제 구현에서는 큰 임베딩 계층 가중치와 원한 인코딩 벡터를 곱하는 거은 매우 비효율적이므로, 다눈히 테이블에서 검색(lookup)하는 작업을 수행 

- word2vec을 사용하여 단어를 임배딩 벡터로 변환한 후 신경망에 직접 넣는 대신 임베딩 계층을 이용하여 원핫 인코딩 벡터를 입력으로 넣도록 구현

## 6.4 word2vec

CBOW와 Skip gram 방법 모두 윈도우의 크기가 주어지면, 특정 단어를 기준으로 윈도우 내의 주변 단어들을 사용하여 단어 임배딩 학습  

CBOW : 신경망은 주변에 나타나는 단어들을 원핫 인코딩 된 벡터로 입력받아 해당 단어 예측  

- CBOW (이미지 출처: 데이터 사이언스 스쿨)

![](https://datascienceschool.net/upfiles/e62aadf1e8324d16a66288f2c83c470a.png)

   

Skip gram : 대상 단어를 원 핫 인코딩 벡터로 입력받아 주변에 나타나는 단어들을 예측하는 네트워크 구성  

Skip gram 상세훈련 방식

- wi가 주어졌을 때 앞뒤 n개의단어($w_{t-n} , ... w_{t_n}$)를 예측하도록 훈련(윈도우 크기는 n)

- $\hat{\theta}=argmax\sum^{T}_{t=1}(\sum^{n}_{t=1}{logP(w_t-i|w_t;\theta) + \sum^{n}_{i=1}logP(w_{t+1}|w_t;\theta)})$

- Skipgram (이미지 출처: 데이터 사이언스 스쿨)  

  ![](https://datascienceschool.net/upfiles/de649c0d600f410dacf09f71639209ed.png)

## 6.5 GloVe

- 대상 단어에 대해서 코퍼스에 함께 나타난 단어별 출현 빈도 예측($x_{ij}$ 는 두 단어 $w_{i},w_{j}$ 의 co-occurrence , $b_i,b_j$ 는 각 단어의 bias)
- $Loss = \sum f(x_{ij}) \times \left( w_i^t w_j + b_i + b_j - log(x_{ij}) \right)^2$
- 단어 $w_i, w_j$의 벡터 곱에 각 단어의 bias인 $b_i, b_j$를 더한 값이 동시 출현 빈도의 log값과 비슷해지도록 $w_i, w_j, b_i, b_j$를 학습함. $b_i, b_j$가 0인 것은 두 단어의 벡터 곱이 동시 출현 빈도의 log값이 되는 것. 동시 출현 빈도가 높을수록 두 단어 벡터는 비슷해져야 함. $b_i, b_j$는 빈번하게 등장하는 단어의 기본 빈도수 역할
- 출현빈도 근사하는 회귀라서 MSE사용 
- 단어의 빈도에 따라 손실함수에 가중치 부여

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Skip gram                                                    | Glove                                                        |
| 코퍼스내에서 주변 단어 예측                                  | 처음에 코퍼스를 통해 단어별 동시 출현 빈도 조사 > 출현 빈도 행렬 > 동시 출현 빈도 근사 |
|                                                              | 빠름                                                         |
| 사전 확률이 낮은(출현 빈도 자체가 적은) 단어의 학습 기회 낮은 | Skip gram  에의해 보완됨                                     |

- 파라미터를 조정하면 성능에 큰 차이는 없음. 주어진 상황에 맞는 방식으로 구현

## 6.6 Word2vec

### 6.6.1 FastText를 활용한 단어 임베딩 벡터 학습

- 빠른 속도를 자랑(Fasttext알고리즘 자체가 매우 빠름)

- 단어 임베딩 기능 + 텍스트 분류 훈련하는 기능

## 6.7 맺음말

- 기존의 선형 차원 축소 방법에 비해 신경망은 비선형적인 차원 축소를 통해 특징을 효율적으로 추출
- 비선형적인 차원 축소는 계신 비용이 비싸고 최적화가 어려움
- word2vec은비선형적인 방법을 사용하지 않고도 좋은 단어 임베딩 구현
- 단어간의 유사도를 데이터 기반으로 효과적이면서도 정확하게 계산할 수 있게 됨.

