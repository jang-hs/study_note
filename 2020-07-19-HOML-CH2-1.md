데이터 프로젝트 진행 단계 예시
1. 큰 그림 보기
2. 데이터 구하기
3. 데이터 탐색 및 시각화
4. 머신러닝 알고리즘 적용을 위한 데이터 준비
5. 모델 선택 및 훈련
6. 모델 상세 조정
7. 솔루션 제시
8. 시스템 론칭 / 모니터링 / 유지보수  
------

- 문제 정의 : '비즈니스의 목적이 정확히 무엇인가' 파악하기 > '현재 솔루션은 어떻게 구성되어 있는가(있다면)'
- 파이프라인 : 데이터 처리 컴포넌트들이 연속되어 있는것. 머신러닝 시스템은 데이터를 조작하고 변환할 일이 많아 파이프라인을 사용하는 일이 매우 흔함.
  
- 성능 측정 지표 선택
	- RMSE(평균 제곱근 오차) : 오차가 커질수록 이 값은 더욱 커지므로 예측에 얼마나 많은 오류가 있는지 가늠하게 해줌(회귀 문제의 전형적 성능 지표)  
	  $RMSE(X,h)=\sqrt { \frac 1 m \sum_{i=1}^{m} h({x}^{(i)}) - y^{(i)})^2 }$
  - MAE(평균 절대 오차) : 이상치로 보이는 구역이 많을 때, RMSE에비해 이상치에 덜 민감  
    $MAE(X,h) = \frac 1 m \sum_{i=1}^{m} |h(x^{(i)}) - y^{(i)}|$  
  
- 데이터 구조 훑어보기
	- `value_counts()` : 범주형 변수에서 카테고리별 갯수를 셀 수 있다.
	- `describe()` : 숫자형 특성의 요약 정보를 보여줌
	- `hist()` : `df.hist()` 사용시 모든 숫자형 특성에 대한 히스토그램 출력

**테스트 세트 생성**  

``` python
import numpy as np
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
    
# 사용예시
train_set, test_set = split_train_test(data, 0.2)
```

데이터셋을 업데이트한 후에도 안정적인 훈련/테스트 분할을 위한 일반적인 해결책은 샘플의 식별자를 사용하여 테스트 세트로 보낼지 말지 결정하는 것. 여러번 반복하여 실행되면서 데이터셋이 갱신되더라도 테스트 세트가 동일하게 유지. 세로운 테스트 세트는 새 샘플의 20%를 갖게 되지만 이전 훈련셋의 샘플은 포함하지 않음.  
``` python
# python2와의 호환을 위함
from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32
def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id : test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
    
# 사용예시
train_set, test_set = split_train_test_by_id(data_with_id, 0.2, 'index')
```

사이킷런에서 제공하는 서브셋 분할 방법(`train_test_split`)    
1) 난수 초깃값을 지정할 수 있는 `random_state`매개변수가 있음  
2) 행의 개수가 같은 여러개의 데이터셋을 넘겨서 같은 인덱스를 기반으로 나눌 수 있음(데이터프레임이 레이블에 따라 여러 개로 나위어 있을 때 유용)      

``` python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(data, test_size = 0.2, random_state = 42)
```

**계층적 샘플링**  
계층이라는 동질의 그룹으로 나누고 테스트 세트가 전체 데이터를 대표하도록 각 계층에서 올바른 수의 샘플을 추출. 계층별로 데이터셋에 충분한 샘플 수가 있어야 함 (`pd.cut()`함수 활용 가능)  

``` python
from sklearn.model_seletion import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)
for train_index, test_index in split.split(data, data['strat']):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
```

> 참고문헌 :  "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow 2e, O'REILLY"