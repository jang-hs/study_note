## 머신러닝 알고리즘을 위한 데이터 준비

머신러닝 알고리즘을 위한 데이터 준비 과정을 함수를 통해 자동화해야하는 이유

- 어떤 데이터셋에 대해서도 데이터 변환을 손쉽게 반복할 수 있음
- 향후 프로젝트에 사용할 수 있는 변환 라이브러리를 점진적으로 구축하게 됨
- 실제 시스템에서 알고리즘에 새 데이터를 주입하기 전에 변환하는데 사용 가능
- 여러가지 데이터 변환을 쉽게 시도해볼 수 있고 어떤 조합이 가장 좋은지 확인하는데 편리

  

### 누락값 대체하는 손쉬운 방법 `SimpleImputer`

``` python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy = "median")
```

  

### 범주형 특성을 다루기 위한 `OrdinalEncoder`

``` python
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]
```

```
array([[0.],
       [0.],
       [4.],
       [1.],
       [0.],
       [1.],
       [0.],
       [1.],
       [0.],
       [0.]])
```

  

한 특성만 1, 나머지는 0인 경우 One-Hot encodeing 사용 - `OneHotEncoder` 클래스 사용

``` python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot
```

``` 
<16512x5 sparse matrix of type '<class 'numpy.float64'>'
	with 16512 stored elements in Compressed Sparse Row format>
```

반환되는 희소 행렬을 배열로 바꾸기 위해서는 `toarray()` 사용

``` python
housing_cat_1hot.toarray()
```

``` 
array([[1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       ...,
       [0., 1., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0.]])
```

  

``` python
cat_encoder.categories_
```

``` 
array([[1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       ...,
       [0., 1., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0.]])
```

  

### 사용자 정의 변환기

``` python
from sklearn.base import BaseEstimator, TransformerMixin

# 열 인덱스
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True): # *args 또는 **kargs 없음
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # 아무것도 하지 않습니다
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.to_numpy())
```

  

### 변환 파이프라인

``` python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)
```



### 교차검증

- k-fold cross validation : 훈련 세트를 폴드(fold) 라고 불리는 k개의 서브셋으로 무작위 분할하고, 그 다음 결정 트리를 k번 훈련하고 평가. 매번 다른 폴드를 선택해서 평가에 사용하고 나머지 k-1개의 폴드는 훈련에 사용한다.



### 모델 세부 튜닝

- 그리드 탐색 : 사이킷런의 `GridSearchCV` 이용. 탐색하고자 하는 하이퍼 파라미터와 시도해볼 값을 지정. 가능한 모든 조합에 대해서 교차 검증을 사용한다.

```python
from sklearn.model_selection import GridSearchCV

param_grid = [
    # 12(=3×4)개의 하이퍼파라미터 조합을 시도합니다.
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    # bootstrap은 False로 하고 6(=2×3)개의 조합을 시도합니다.
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor(random_state=42)
# 다섯 개의 폴드로 훈련하면 총 (12+6)*5=90번의 훈련이 일어납니다.
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)

grid_search.best_params_ # 최상의 파라미터 조합
grid_search.best_estimator_ # 최상의 모델
```

  

- 랜덤 탐색 : 사이킷런의 `RandomizedSearchCV` 이용. 그리드 서치와 비슷하지만 각 반복마다 하이퍼파라미터에 임의의 수를 대입하여 지정한 횟수만큼 평가한다. 
  - 100회 실시할 경우 하이퍼파라미터마다 각기 다른 100개의 값을 탐색
  - 단순히 반복 횟수를 조절하는 것으로 컴퓨팅 자원을 제어 가능

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {
        'n_estimators': randint(low=1, high=200),
        'max_features': randint(low=1, high=8),
    }

forest_reg = RandomForestRegressor(random_state=42)
rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,
                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
rnd_search.fit(housing_prepared, housing_labels)
```

  

-  앙상블 탐색 : 최상의 모델을 연결해보는 것













`









> 참고문헌 :  "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow 2e, O'REILLY"